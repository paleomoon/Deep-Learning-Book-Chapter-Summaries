# 前言

目前为止深度学习已经经历了三次发展浪潮：

1. 20 世纪40 年代到60 年代深度学习的雏形出现在控制论（cybernetics）中，，随着生物学习理论的发展和第一个模型的实现（如感知机），能实现单个神经元的训练。
2. 20 世纪80 年代到90 年代深度学习表现为联结主义（connectionism），可以使用反向传播训练具有一两个隐藏层的神经网络。
3. 2006 年，才真正以深度学习之名复兴。

现代深度学习的最早前身是从神经科学的角度出发的简单线性模型。线性模型无法学习异或（XOR）函数。从这一时期发展出了随机梯度下降（stochastic gradient descent）。

神经科学被视为深度学习研究的一个重要灵感来源，发展出了整流线性单元（rectified linear unit）等结构。

联结主义的中心思想是，当网络将大量简单的计算单元连接在一起时可以实现智能行为。

分布式表示（distributed representation）其思想是：系统的每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能输入的表示。分布式表示的概念是本书的核心。

截至2016 年，一个粗略的经验法则是，监督深度学习算法在每类给定约5000 个标注样本情况下一般将达到可以接受的性能，当至少有1000 万个标注样本的数据集用于训练时，它将达到或超过人类表现。

自从引入隐藏单元，人工神经网络的大小大约每2.4 年翻一倍。

神经图灵机能学习读取存储单元和向存储单元写入任意内容。这样的神经网络可以从期望行为的样本中学习简单的程序。例如，从杂乱和排好序的样本中学习对一系列数进行排序。

在强化学习（reinforcement learning）中，一个自主的智能体必须在没有人类操作者指导的情况下，通过试错来学习执行任务。

# 线性代数

张量（tensor）：在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们称之为张量。